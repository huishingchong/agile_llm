{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "### Import Packages and API keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch pinecone-client langchain-community faiss-cpu sentence-transformers\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "huggingface_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "if not huggingface_api_token:\n",
    "    huggingface_api_token = getpass(\"Enter your Hugging Face Hub API token: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b-instruct\")\n",
    "\n",
    "# I will be using T5 model from open source huggingface library\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "# model_name = \"meta-llama/Llama-2-7b\"\n",
    "# model_name = \"google/flan-t5-xxl\"\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\"temperature\":0.5, \"max_length\":1024, \"max_new_tokens\":200})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template-based Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be using Langchain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "# pipeline = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=200,\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "\n",
    "template= \"\"\"\n",
    "Try to be helpful as you can in a Computer Science context.\n",
    "Question: {question}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "# # llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# # llm_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "# def chat_interface(textbox, chat):\n",
    "#     input_dict = {'question': textbox}\n",
    "#     response = llm_chain.run(input_dict)\n",
    "\n",
    "#     print(\"user:\", textbox)\n",
    "#     print(\"bot:\", response)\n",
    "#     return response\n",
    "\n",
    "def chat_interface(textbox, chat):\n",
    "    input_dict = {'question': textbox}\n",
    "    response_dict = llm_chain.invoke(input_dict)\n",
    "    text = response_dict['text']  # Extract the text from the dictionary\n",
    "    # Split the text based on \"Response:\" and extract the part after it\n",
    "    response_text = text.split(\"Response:\")[1].strip()\n",
    "    return response_text\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=chat_interface,\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me a question\", container=False, scale=7),\n",
    "    title=\"Chatbot\",\n",
    "    description=\"Ask Chatbot any question\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\"What does AI stand for?\", \"What is Software Engineering?\", \"What is Cybersecurity?\"],\n",
    "    cache_examples=False,\n",
    "    retry_btn=None,\n",
    "    undo_btn=\"Delete Previous\",\n",
    "    clear_btn=\"Clear\",\n",
    ").launch()\n",
    "\n",
    "#I am a final year Computer Science student seeking to find a graduate role in __. What are practical skills required for a career in __?\n",
    "#I am a beginner that wants to get into __, where should I start?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Evaluating the model with prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standardized test set\n",
    "    # IT Consultant, Cloud Engineer...\n",
    "\n",
    "# ROGUE? BLUE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG from synthetic data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:06<00:00, 63.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use langchain packages to help with implementing retrieval augmentation generation\n",
    "from datasets import load_dataset\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "loader = CSVLoader(file_path=\"rag_sample.csv\")\n",
    "documents = loader.load()  # Load data for retrieval\n",
    "\n",
    "# Step 2: Split Documents\n",
    "text_split = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=150)\n",
    "# text_split = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "d = text_split.split_documents(documents)\n",
    "\n",
    "# Step 3: Create a FAISS Index\n",
    "model_name = \"sentence-transformers/gtr-t5-base\"\n",
    "modelPath = model_name\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")\n",
    "\n",
    "llm = SentenceTransformer('sentence-transformers/gtr-t5-base')\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "db = FAISS.from_documents(d, embedding=embeddings)\n",
    "\n",
    "# Step 5: Perform RAG\n",
    "template= \"\"\"\n",
    "Try to be helpful as you can in a Computer Science context.\n",
    "Question: {question}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Specify the model name you want to use\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "# model_name = \"meta-llama/Llama-2-7b\"\n",
    "# model_name = \"google/flan-t5-xxl\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a text generation pipeline using the model and tokenizer\n",
    "hf = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_name, \n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=hf,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512, \"max_new_tokens\": 20},\n",
    ")\n",
    "\n",
    "# hf = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_name,\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs={\"max_new_tokens\": 10},\n",
    "# )\n",
    "\n",
    "# llm_chain = (\n",
    "#     {\n",
    "#         \"context\": itemgetter(\"question\") | db.as_retriever(),\n",
    "#         \"question\": itemgetter(\"question\"),\n",
    "#     }\n",
    "#     | prompt\n",
    "#     | hf\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "# chain = RetrievalQA.from_chain_type(\n",
    "#         llm=llm,\n",
    "#         chain_type=chain_type,\n",
    "#         retriever=docsearch.as_retriever(),\n",
    "#         return_source_documents=True,\n",
    "#         chain_type_kwargs={\"prompt\":prompt}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 357, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1177, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/gradio/utils.py\", line 662, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/gradio/chat_interface.py\", line 460, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/w9/7rc799t12djb17hk8vzj9ykw0000gn/T/ipykernel_25114/3535185019.py\", line 14, in chat_interface\n",
      "    result = qa.run({\"query\": question})\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 538, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 363, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 162, in invoke\n",
      "    raise e\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py\", line 144, in _call\n",
      "    answer = self.combine_documents_chain.run(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 543, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 363, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 162, in invoke\n",
      "    raise e\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py\", line 136, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "                                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/combine_documents/refine.py\", line 152, in combine_docs\n",
      "    res = self.initial_llm_chain.predict(callbacks=callbacks, **inputs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/llm.py\", line 293, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 363, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 162, in invoke\n",
      "    raise e\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/llm.py\", line 103, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain/chains/llm.py\", line 115, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 568, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 741, in generate\n",
      "    output = self._generate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 605, in _generate_helper\n",
      "    raise e\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 592, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 227, in _generate\n",
      "    text = response[\"generated_text\"][len(batch_prompts[j]) :]\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'generated_text'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_interface(textbox, chat):\n",
    "    # input_dict = {'question': textbox}\n",
    "    # # response = llm_chain.invoke(input_dict)\n",
    "    # # response_text = response['text'].split(\"Response:\")[1].strip()\n",
    "    # result = qa.run({\"query\": question})\n",
    "    # print(result[\"result\"])\n",
    "    # return result[\"result\"]\n",
    "    # Access the user's question from the textbox parameter\n",
    "    question = textbox\n",
    "    \n",
    "    # Run the QA pipeline with the user's question\n",
    "    result = qa.run({\"query\": question})\n",
    "    \n",
    "    # Extract and return the response from the QA result\n",
    "    response_text = result[\"result\"]\n",
    "    return response_text\n",
    "# def chat_interface(textbox, chat):\n",
    "#     docs = db.similarity_search(textbox)\n",
    "#     vectorstore = FAISS.from_texts(\n",
    "#     [textbox], embedding=embeddings\n",
    "#     )\n",
    "#     retriever = vectorstore.as_retriever()\n",
    "#     # input_dict = {'question': textbox, 'input_documents': docs }\n",
    "#     input_dict = {'question': textbox}\n",
    "#     response = llm_chain.invoke(input_dict)\n",
    "#     response_text = text.split(\"Response:\")[1].strip()\n",
    "#     return response_text\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=chat_interface,\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me a question\", container=False, scale=7),\n",
    "    title=\"Chatbot\",\n",
    "    description=\"Ask Chatbot any question\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\"What does AI stand for?\", \"What is Software Engineering?\", \"What is Cybersecurity?\"],\n",
    "    cache_examples=False,\n",
    "    retry_btn=None,\n",
    "    undo_btn=\"Delete Previous\",\n",
    "    clear_btn=\"Clear\",\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAYBE DO THIS FIRST? AND SEE THE DOWNSIDE, AND LEARN THAT IT IS NOT REQUIRED (doesn't solve hallucinations and timely context!)\n",
    "# Fine-tune with input and output example data sets\n",
    "\n",
    "# Compare with different models (one fine-tuned one just pre-trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "# Find that fine-tuning is not needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full adapted model (combined of all approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge retrieved\n",
    "# Augmented Prompt\n",
    "# Fine-tuned/pre-trained LLM\n",
    "\n",
    "import gradio as gr\n",
    "def chat_interface(textbox, chat):\n",
    "    # docs = db.similarity_search(textbox)\n",
    "    # input_dict = {'question': textbox, 'input_documents': docs }\n",
    "    input_dict = {'question': textbox}\n",
    "    response = llm_chain.run(input_dict)\n",
    "\n",
    "    print(\"user:\", textbox)\n",
    "    print(\"bot:\", response)\n",
    "    return response\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=chat_interface,\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me a question\", container=False, scale=7),\n",
    "    title=\"Chatbot\",\n",
    "    description=\"Ask Chatbot any question\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\"What does AI stand for?\", \"What is Software Engineering?\", \"What is Cybersecurity?\"],\n",
    "    cache_examples=False,\n",
    "    retry_btn=None,\n",
    "    undo_btn=\"Delete Previous\",\n",
    "    clear_btn=\"Clear\",\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
