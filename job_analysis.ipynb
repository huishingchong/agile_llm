{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "### Import Packages and API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch torchvision torchaudio langchain-community faiss-cpu sentence-transformers langchain gradio mlflow evaluate langchain_experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "huggingface_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "if not huggingface_api_token:\n",
    "    huggingface_api_token = getpass(\"Enter your Hugging Face Hub API token: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=model_name,\n",
    "    model=model_name,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.5,\n",
    "    # max_length:1024,\n",
    "    max_new_tokens=200\n",
    ")\n",
    "# llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\"temperature\":0.5, \"max_length\":1024, \"max_new_tokens\":200})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG from synthetic data set (job analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up documents, embeddings and retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use langchain packages to help with implementing retrieval augmentation generation\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from datasets import load_dataset\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gradio as gr\n",
    "from requests import get\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up documents, embeddings and retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROMPT1: QA\n",
    "template = \"\"\"Use the following context to answer the question at the end.\n",
    "If you don't know the answer, please think rationally and answer from your own knowledge base.\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "#PROMPT2: Normal prompting\n",
    "template= \"\"\"\n",
    "        Please answer the question.\n",
    "        Answer professionally, and where appropriate, in a Computer Science educational context.\n",
    "        Question: {question}\n",
    "        Response:\n",
    "        \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"sentence-transformers/gtr-t5-base\" # Using t5 sentence transformer model to generate embeddings\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # Normalizing embeddings may help improve similarity metrics by ensuring that embeddings magnitude does not affect the similarity scores\n",
    "\n",
    "# Initialise an instance of HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job(query):\n",
    "    \"\"\"Returns the subject of the sentence, helper function to feed into job search.\"\"\"\n",
    "    helper_template = \"\"\"\n",
    "    Sentence: {query}\n",
    "    Output only the subject of the sentence, give one or two words.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=helper_template, input_variables=[\"query\"])\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=model_name,\n",
    "        model=model_name,\n",
    "        task=\"text-generation\",\n",
    "        temperature=0.5,\n",
    "        # max_length:1024,\n",
    "        max_new_tokens=200\n",
    "    )\n",
    "    helper_llm = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "    print(helper_llm)\n",
    "    response = helper_llm.invoke(input=query)\n",
    "    text = response[\"text\"]\n",
    "    # if \"\\n\" in text:\n",
    "    #     text = text.split(\"\\n\")[1].strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reed_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reed_key = os.getenv('REED_API_KEY')\n",
    "BASE_URL = 'https://www.reed.co.uk/api/1.0/search'\n",
    "\n",
    "def create_jobs_csv(job_name, location):\n",
    "    # Construct the request URL\n",
    "    job_name = cleanhtml(job_name)\n",
    "    search_url = f'{BASE_URL}?keywords={job_name}&locationName={location}'\n",
    "    # Send the request\n",
    "    search_response = get(search_url, auth=(reed_key, '')) # authentication header as the username, with the password left empty\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if search_response.status_code == 200:\n",
    "        job_listings = search_response.json()\n",
    "\n",
    "        # Create or overwrite the CSV file\n",
    "        with open('job_listings.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['Job Title', 'Job Description', 'Location', 'Part-time', 'Full-time', 'Graduate', 'Minimum Salary']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Iterate through job listings\n",
    "            for job in job_listings[\"results\"]:\n",
    "                len(job_listings[\"results\"])\n",
    "                job_id = job[\"jobId\"]\n",
    "                details_url = f'https://www.reed.co.uk/api/1.0/jobs/{job_id}'\n",
    "                detail_response = get(details_url, auth=(reed_key, ''))\n",
    "                detail = detail_response.json()\n",
    "                job_title = detail.get(\"jobTitle\", \"\")\n",
    "                job_description = cleanhtml(detail.get(\"jobDescription\", \"\"))\n",
    "                location = detail.get(\"locationName\", \"\")\n",
    "                graduate = detail.get(\"graduate\", \"\")\n",
    "                keywords = detail.get(\"keywords\", \"\")\n",
    "                part_time = detail.get(\"partTime\", \"\")\n",
    "                full_time = detail.get(\"fullTime\", \"\")\n",
    "                min_salary = detail.get(\"minimumSalary\", \"\")\n",
    "                # Write job details to CSV\n",
    "                writer.writerow({'Job Title': job_title, 'Job Description': job_description, 'Location': location, \"Part-time\": part_time, \"Full-time\": full_time, 'Graduate': graduate, 'Minimum Salary': min_salary})\n",
    "    else:\n",
    "        print(f'Error: {search_response.status_code}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(query):\n",
    "    subject = get_job(query) #find keywords to search jobs in API\n",
    "    if subject != \"\":\n",
    "        create_jobs_csv(subject, \"london\")\n",
    "\n",
    "        loader = CSVLoader(file_path=\"job_listings.csv\")\n",
    "        documents = loader.load() # load data for retrieval\n",
    "\n",
    "        d = text_split.split_documents(documents)\n",
    "        db = FAISS.from_documents(d, embeddings)\n",
    "\n",
    "        chain_type_kwargs = {\"prompt\": QA_CHAIN_PROMPT}\n",
    "        qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "            retriever=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5}),  #ANY PARAMETERS? e.g. search_kwargs={k: 3}\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs=chain_type_kwargs, verbose=True)\n",
    "        input_dict = {'query': textbox}\n",
    "        result = qa.invoke(input_dict)\n",
    "        documents = result.get(\"source_documents\", [])\n",
    "        for i in documents:\n",
    "            print (i)\n",
    "        text = result['result']\n",
    "        return documents, text\n",
    "    else: # If no jobs are found, normal prompting and response is done\n",
    "        print(\"TAKING ELSE ROUTE\")\n",
    "        llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "        input_dict = {'question': textbox}\n",
    "        response_dict = llm_chain.invoke(input_dict)\n",
    "        response = response_dict['text'].split(\"Response:\")[1].strip()\n",
    "        return [], response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN GOOGLE COLLAB WITH GPU!\n",
    "!pip install torch tiktoken mlflow bert_score textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO REVISE\n",
    "data_cs_industry = pd.DataFrame({\n",
    "    \"question\": [\n",
    "        \"What are the latest trends in artificial intelligence and machine learning?\",\n",
    "        \"What are some emerging programming languages that are gaining popularity in the industry?\",\n",
    "        \"I am a beginner that wants to get into Data Science, where should I start?\",\n",
    "        \"I am a final-year Computer Science student wanting to find a graduate role in Cybersecurity. What are the practical skills required for a career in Cybersecurity that are currently in-demand?\",\n",
    "        \"What are the essential skills required for a career in cybersecurity?\",\n",
    "        \"What are some in-demand technical skills for aspiring data analysts?\",\n",
    "        \"What are the career prospects for individuals with expertise in cybersecurity risk management?\",\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"In the realm of artificial intelligence (AI) and machine learning (ML), several notable trends have emerged recently. Firstly, there's a growing focus on explainable AI (XAI), which aims to make AI models more transparent and understandable to humans, crucial for applications in fields like healthcare and finance where interpretability is paramount. Secondly, federated learning has gained traction, enabling training of ML models across decentralized devices while preserving data privacy, pivotal for IoT and edge computing scenarios. Additionally, reinforcement learning (RL) advancements, particularly in deep RL, have seen remarkable progress, empowering AI systems to make sequential decisions in dynamic environments, with applications spanning robotics, autonomous vehicles, and gaming. Lastly, the integration of AI with other technologies like blockchain for enhanced security and trustworthiness and with quantum computing for tackling complex optimization problems signifies promising directions for future research and innovation in the AI landscape.\",\n",
    "        \"Several emerging programming languages are gaining traction in the industry due to their unique features and capabilities. One such language is Rust, known for its emphasis on safety, concurrency, and performance, making it suitable for systems programming where reliability and efficiency are critical. Another language on the rise is Julia, which specializes in numerical and scientific computing, offering high performance comparable to traditional languages like C and Fortran while maintaining a user-friendly syntax and extensive library support. Additionally, Kotlin, a statically typed language interoperable with Java, has become increasingly popular for Android app development, offering modern features and improved developer productivity. Lastly, Swift, developed by Apple, has gained momentum for iOS and macOS development, providing a concise and expressive syntax along with powerful features like optionals and automatic memory management. These emerging languages cater to specific niches and address evolving industry needs, showcasing their growing relevance and adoption in the programming landscape.\",\n",
    "        \"Here is a few things to learn about Data Science to get you started: \\nLearn Python or R: Choose one as your primary programming language. \\nBasic Statistics: Understand mean, median, mode, standard deviation, and probability.\\nData Manipulation: Learn Pandas (Python) or dplyr (R) for data cleaning and manipulation.\\nData Visualization: Use Matplotlib, Seaborn (Python), or ggplot2 (R) for visualization.\\nMachine Learning Basics: Start with linear regression, logistic regression, decision trees, and evaluation metrics.\\nPractice: Work on projects using real-world datasets from sources like Kaggle.\\nStay Updated: Follow online resources and communities for the latest trends and techniques.\",\n",
    "        \"As a final-year Computer Science student aiming for a graduate role in cybersecurity, it's essential to focus on developing practical skills that are currently in high demand in the industry.\\nSome of these key skills include:\\n\\n1. Knowledge of Networking: Understanding networking fundamentals, protocols (such as TCP/IP), and network architecture is crucial for identifying and mitigating security threats. Familiarize yourself with concepts like firewalls, routers, VPNs, and intrusion detection systems (IDS).\\n\\n2. Proficiency in Operating Systems: Gain proficiency in operating systems such as Linux and Windows, including command-line operations, system administration tasks, and security configurations. Being able to secure and harden operating systems is essential for protecting against common cybersecurity threats.\\n\\n3. Understanding of Cryptography: Cryptography is at the heart of cybersecurity, so having a solid understanding of encryption algorithms, cryptographic protocols, and cryptographic techniques is vital. Learn about symmetric and asymmetric encryption, digital signatures, hashing algorithms, and their applications in securing data and communications.\\n\\n4. Penetration Testing and Ethical Hacking: Develop skills in penetration testing and ethical hacking to identify vulnerabilities and assess the security posture of systems and networks. Familiarize yourself with tools and techniques used by ethical hackers, such as Kali Linux, Metasploit, Nmap, and Wireshark.\\n\\n5. Security Assessment and Risk Management: Learn how to conduct security assessments, risk assessments, and threat modeling to identify, prioritize, and mitigate security risks effectively. Understand risk management frameworks like NIST, ISO 27001, and COBIT, and how to apply them in real-world scenarios.\\n\\n6. Incident Response and Forensics: Acquire knowledge of incident response procedures, including detection, analysis, containment, eradication, and recovery from security incidents. Understand digital forensics principles and techniques for investigating and analyzing security breaches and cybercrimes.\\n\\n7. Security Awareness and Communication: Develop strong communication skills to effectively convey cybersecurity concepts, risks, and recommendations to technical and non-technical stakeholders. Being able to raise awareness about cybersecurity best practices and policies is essential for promoting a security-conscious culture within organizations.\\n\\n8. Continuous Learning and Adaptability: Cybersecurity is a rapidly evolving field, so it's essential to cultivate a mindset of continuous learning and adaptability. Stay updated with the latest threats, trends, technologies, and best practices through professional development, certifications, and participation in cybersecurity communities and events.\\n\\nBy focusing on developing these practical skills and staying abreast of industry trends and advancements, you'll be well-prepared to pursue a successful career in cybersecurity upon graduation. Additionally, consider obtaining relevant certifications such as CompTIA Security+, CEH (Certified Ethical Hacker), CISSP (Certified Information Systems Security Professional), or others to further enhance your credentials and marketability in the field.\",\n",
    "        \"A career in cybersecurity require a broad spectrum of practical skills, including proficiency in network security protocols and tools like firewalls and intrusion detection/prevention systems (IDS/IPS) for safeguarding network infrastructure. Secure coding practices and knowledge of common vulnerabilities are essential for developing secure software applications, with expertise in frameworks like OWASP Top 10 aiding in vulnerability mitigation. Encryption techniques and cryptographic protocols are vital for securing sensitive data, while incident response and digital forensics skills, alongside tools like SIEM systems, enable effective threat detection and response. Proficiency in penetration testing frameworks like Metasploit and security assessment tools is crucial for identifying and remediating security weaknesses, while knowledge of compliance frameworks such as GDPR ensures organizational adherence to cybersecurity regulations. Effective communication and collaboration skills are imperative for conveying cybersecurity risks and recommendations to stakeholders and collaborating with cross-functional teams to implement security measures. Continued learning and staying updated with the latest cybersecurity trends and technologies are key for navigating this ever-evolving field successfully.\",\n",
    "        \"In-demand technical skills for data analysts include proficiency in programming languages like Python, R, or SQL for data manipulation, analysis, and visualization. Familiarity with statistical analysis techniques, such as regression analysis, hypothesis testing, and predictive modeling, is essential for deriving insights from data. Knowledge of data querying and database management systems like MySQL, PostgreSQL, or MongoDB is valuable for accessing and organizing large datasets. Expertise in data wrangling techniques, using tools like pandas, dplyr, or data.table, enables cleaning and transforming raw data into actionable insights. Proficiency in data visualization libraries like Matplotlib, ggplot2, or seaborn is crucial for creating informative and visually appealing charts, graphs, and dashboards to communicate findings effectively. Additionally, experience with machine learning frameworks like scikit-learn or TensorFlow, along with knowledge of data mining techniques, enhances the ability to build predictive models and extract patterns from data.\",\n",
    "        \"Career opportunities for individuals in cybersecurity risk management include roles such as cybersecurity risk analysts, security consultants, risk managers, compliance officers, and cybersecurity architects. These professionals play a critical role in identifying, evaluating, and prioritizing cybersecurity risks, developing risk mitigation strategies, and ensuring compliance with regulatory requirements and industry standards. With the ever-evolving threat landscape and the increasing complexity of cybersecurity challenges, individuals with expertise in cybersecurity risk management can expect to have a wide range of career opportunities and advancement prospects in both the public and private sectors, including government agencies, financial institutions, healthcare organizations, and consulting firms. Additionally, obtaining relevant certifications such as Certified Information Systems Security Professional (CISSP), Certified Information Security Manager (CISM), or Certified Risk and Information Systems Control (CRISC) can further enhance career prospects and credibility in the field.\",\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_test = pd.DataFrame({\n",
    "    \"question\": [\n",
    "        \"What are qualifications for Data Scientist?\",\n",
    "        \"Can you recommend me a graduate role for Software Engineering?\",\n",
    "        \"What are the job responsibility of a Computer Vision Engineer?\",\n",
    "        \"Graduate role...\",\n",
    "        \"Intern...\",\n",
    "        \"Can you name and describe the practical skills a full stack engineer should have?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"The qualifications for Data Scientist include a background in Computer Science, Engineering, or a related field, proficiency in Python programming, machine learning libraries, commercial experience with Azure, R, SQL, PowerBI, and Tableau, machine learning, assimilation, and optimizing data visualization dashboards.\",\n",
    "        \"\",\n",
    "        \"A Computer Vision Engineer is responsible for designing, developing, and optimizing computer vision algorithms for SLAM applications in construction environments. They implement and integrate SLAM solutions into their autonomous machinery and on-site monitoring systems. They collaborate with cross-functional teams to understand project requirements and define technical specifications. They conduct research and experimentation to evaluate and adopt the latest advancements in computer vision technology. They perform rigorous testing and validation to ensure the reliability and performance of developed solutions in real-world construction.\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"As a full stack engineer, it is important to have a solid understanding of the entire development process, from front-end to back-end. You should be proficient in HTML, CSS, and JavaScript for web development, as well as knowledge of database management and software design. Additionally, you should be familiar with the latest trends and technologies, such as React, Angular, and Vue.js for front-end development, and Node.js and Python for back-end development.\",\n",
    "        \n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: To prevent having to run predictions again, save output to the dataset as a new column 'predictions'\n",
    "import datasets\n",
    "\n",
    "def chain_predictions_from_data(eval_data):\n",
    "  data = eval_data.copy()\n",
    "  predictions = []  # List to store predictions for all questions\n",
    "  source_documents = []\n",
    "  for question in data['question']:\n",
    "    # Invoke the llm_chain model with the current question\n",
    "    documents, response = pipeline(question)\n",
    "    predictions.append(response)  # Append the generated text to the predictions list\n",
    "    print(question)\n",
    "    print(documents)\n",
    "    print(response)\n",
    "    # documents = response.get(\"source_documents\", [])\n",
    "    concatenated_page_content = \"\"\n",
    "    for document in documents:\n",
    "    # Append the page_content of each Document to the page_contents array\n",
    "      concatenated_page_content += document.page_content + \"\\n\"\n",
    "    source_documents.append(concatenated_page_content)\n",
    "  data['source_documents'] = source_documents\n",
    "  data['predictions'] = predictions  # Assign the predictions list to a new column\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_data_cs_industry = chain_predictions_from_data(data_cs_industry)\n",
    "os.makedirs(\"industry\", exist_ok=True)\n",
    "rag_data_cs_industry.to_csv(\"industry/rag.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "def generate_bert_results_table(eval_data):\n",
    "    ref_texts = eval_data['ground_truth']\n",
    "    predictions = eval_data['predictions']\n",
    "\n",
    "    # Compute BERTScore\n",
    "    bertscore = load(\"bertscore\")\n",
    "    results = bertscore.compute(predictions=predictions, references=ref_texts, model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "    # Create DataFrame from BERTScore results\n",
    "    bert_results_table = pd.DataFrame(results, index=range(0, len(results['precision'])))\n",
    "    bert_results_table['question'] = eval_data['question']\n",
    "\n",
    "    bert_results_table.drop(columns=['hashcode'], inplace=True)\n",
    "    bert_results_table = bert_results_table[['question', 'precision', 'recall', 'f1']]\n",
    "\n",
    "    return bert_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bert_results_table(rag_data_cs_industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UniEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maszhongming/UniEval.git\n",
    "%cd UniEval\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "nltk.download('punkt')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factual consistency score\n",
    "task = 'fact'\n",
    "\n",
    "src_list = rag_data_skills_build[\"source_documents\"]\n",
    "output_list = rag_data_skills_build[\"predictions\"]\n",
    "\n",
    "# Prepare data for pre-trained evaluators\n",
    "data = convert_to_json(output_list=output_list, src_list=src_list)\n",
    "# Initialize evaluator for a specific task\n",
    "evaluator = get_evaluator(task)\n",
    "# Get factual consistency scores\n",
    "eval_scores = evaluator.evaluate(data, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task = 'dialogue'\n",
    "\n",
    "# a list of dialogue histories\n",
    "src_list = rag_data_skills_build[\"question\"]\n",
    "# a list of additional context that should be included into the generated response\n",
    "context_list = rag_data_skills_build[\"source_documents\"]\n",
    "# a list of model outputs to be evaluated\n",
    "output_list = rag_data_skills_build[\"predictions\"]\n",
    "\n",
    "data = convert_to_json(output_list=output_list,\n",
    "                       src_list=src_list, context_list=context_list)\n",
    "evaluator = get_evaluator(task)\n",
    "eval_scores = evaluator.evaluate(data, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import pandas as pd\n",
    "\n",
    "input_columns = [{\"question\": \"string\"}]\n",
    "output_columns = [{\"text\": \"string\"}]\n",
    "signature = infer_signature(input_columns, output_columns)\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"/falcon-evaluation\")\n",
    "\n",
    "# eval_data: dataframe with questions, ground_truth and predictions\n",
    "def mlflow_evaluation(eval_data):\n",
    "  with mlflow.start_run() as run:\n",
    "    mlflow_results = mlflow.evaluate(\n",
    "        data=eval_data,\n",
    "        targets=\"ground_truth\",\n",
    "        predictions=\"predictions\",\n",
    "        model_type=\"question-answering\",\n",
    "    )\n",
    "    # print(f\"See aggregated evaluation results below: \\n{mlflow_results.metrics}\")\n",
    "\n",
    "    eval_table = mlflow_results.tables[\"eval_results_table\"]\n",
    "    print(f\"See evaluation table below: \\n{eval_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_evaluation(rag_data_cs_industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware specs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
