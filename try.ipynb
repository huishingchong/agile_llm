{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch langchain-community faiss-cpu sentence-transformers\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "huggingface_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "if not huggingface_api_token:\n",
    "    huggingface_api_token = getpass(\"Enter your Hugging Face Hub API token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huishingchong/agile_llm/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'frontend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n",
      "File \u001b[0;32m~/agile_llm/venv/lib/python3.11/site-packages/fitz/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfrontend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mop\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'frontend'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import fitz\n",
    "import torch\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from datasets import load_dataset\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "\n",
    "# Add Text to Chat History\n",
    "def add_text(history, text):\n",
    "    if not text:\n",
    "        raise gr.Error('Enter text')\n",
    "    history.append((text, ''))\n",
    "    return history\n",
    "\n",
    "# Create Prompt Template\n",
    "def create_prompt_template():\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Answer professionally, to the best of your ability, and where appropriate, in a Computer Science educational context.\n",
    "    Use the context and be specific as you can.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    return PromptTemplate.from_template(QA_CHAIN_PROMPT)\n",
    "\n",
    "# Load Embeddings\n",
    "def load_embeddings(model_name):\n",
    "    return HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Load Vector Database\n",
    "def load_vectordb(documents, embeddings):\n",
    "    return FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Load Tokenizer\n",
    "def load_tokenizer(model_name):\n",
    "    return AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load Model\n",
    "def load_model(model_name):\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float32,\n",
    "        token=True,\n",
    "        load_in_8bit=False\n",
    "    )\n",
    "\n",
    "# Create Pipeline\n",
    "def create_pipeline(model, tokenizer):\n",
    "    pipe = pipeline(\n",
    "        model=model,\n",
    "        task='text-generation',\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=200\n",
    "    )\n",
    "    return HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create Conversational Retrieval Chain\n",
    "def create_chain(llm, retriever, prompt):\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True, chain_type_kwargs={\"prompt\": prompt})\n",
    "    return qa\n",
    "\n",
    "# Process File\n",
    "def process_file(file_name, model_name):\n",
    "    prompt = create_prompt_template()\n",
    "    loader = CSVLoader(file_name)\n",
    "    documents = loader.load()  \n",
    "\n",
    "    modelPath = \"sentence-transformers/gtr-t5-base\" # Use a t5 sentence transformer model that maps sentences & paragraphs to a 768 dimensional dense vector space\n",
    "    model_kwargs = {'device':'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': True} # Normalizing embeddings can help improve similarity metrics by ensuring that embeddings magnitude does not affect the similarity scores\n",
    "\n",
    "    # Initialize an instance of HuggingFaceEmbeddings\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    "    )\n",
    "\n",
    "    db = FAISS.from_documents(d, embedding=embeddings)\n",
    "    retriever = db.as_retriever()\n",
    "    # vectordb = load_vectordb(documents, embeddings)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\"temperature\":0.5, \"max_length\":1024, \"max_new_tokens\":200})\n",
    "    \n",
    "    chain = create_chain(llm, retriever, prompt)\n",
    "    return chain, documents\n",
    "\n",
    "# Generate Response\n",
    "def generate_response(history, query, chain, documents, page):\n",
    "    if not query:\n",
    "        raise gr.Error(message='Submit a question')\n",
    "    result = chain({\"question\": query, 'chat_history': history}, return_only_outputs=True)\n",
    "    history.append((query, result[\"answer\"]))\n",
    "    page = list(result['source_documents'][0])[1][1]['page']\n",
    "    return history, page\n",
    "\n",
    "# Render File\n",
    "def render_file(file, page):\n",
    "    doc = fitz.open(file.name)\n",
    "    page = doc[page]\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(300 / 72, 300 / 72))\n",
    "    image = Image.frombytes('RGB', [pix.width, pix.height], pix.samples)\n",
    "    return image\n",
    "\n",
    "# Create Gradio Interface\n",
    "def create_demo():\n",
    "    with gr.Blocks(title= \"RAG Chatbot Q&A\",\n",
    "        theme = \"Soft\"\n",
    "        ) as demo:\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                chat_history = gr.Chatbot(value=[], elem_id='chatbot', height=680)\n",
    "                show_img = gr.Image(label='Overview', height=680)\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=0.60):\n",
    "                text_input = gr.Textbox(\n",
    "                    show_label=False,\n",
    "                    placeholder=\"Type here to ask your PDF\",\n",
    "                container=False)\n",
    "\n",
    "            with gr.Column(scale=0.20):\n",
    "                submit_button = gr.Button('Send')\n",
    "\n",
    "            with gr.Column(scale=0.20):\n",
    "                uploaded_pdf = gr.UploadButton(\"üìÅ Upload PDF\", file_types=[\".pdf\"])\n",
    "                \n",
    "\n",
    "        return demo, chat_history, show_img, text_input, submit_button, uploaded_pdf\n",
    "\n",
    "# Set up event handlers\n",
    "def set_event_handlers(demo, chat_history, show_img, text_input, submit_button, uploaded_pdf):\n",
    "    with demo:\n",
    "        # Event handler for uploading a PDF\n",
    "        uploaded_pdf.upload(renderfile, inputs=[uploaded_pdf], outputs=[show_img])\n",
    "\n",
    "        # Event handler for submitting text and generating response\n",
    "        submit_button.click(add_text, inputs=[chat_history, text_input], outputs=[chat_history], queue=False).\\\n",
    "            success(generate_response, inputs=[chat_history, text_input, uploaded_pdf], outputs=[chat_history, text_input]).\\\n",
    "            success(render_file, inputs=[uploaded_pdf], outputs=[show_img])\n",
    "\n",
    "# Launch the application\n",
    "def launch_app(demo):\n",
    "    demo.queue()\n",
    "    demo.launch()\n",
    "\n",
    "# Main execution flow\n",
    "def main():\n",
    "    demo, chat_history, show_img, text_input, submit_button, uploaded_pdf = create_demo()\n",
    "    # pdf_chatbot = PDFChatBot()\n",
    "    set_event_handlers(demo, chat_history, show_img, text_input, submit_button, uploaded_pdf)\n",
    "    launch_app(demo)\n",
    "\n",
    "# Execute the main function\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
