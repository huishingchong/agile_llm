{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "### Import Packages and API keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch langchain-community faiss-cpu sentence-transformers\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "huggingface_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "if not huggingface_api_token:\n",
    "    huggingface_api_token = getpass(\"Enter your Hugging Face Hub API token: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b-instruct\")\n",
    "\n",
    "# I will be using T5 model from open source huggingface library\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "# model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\"temperature\":0.5, \"max_length\":1024, \"max_new_tokens\":200})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template-based prompting with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be using Langchain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "template= \"\"\"\n",
    "Please answer the question.\n",
    "Answer professionally, and where appropriate, in a Computer Science educational context.\n",
    "Question: {question}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "# # llm_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "# def chat_interface(textbox, chat):\n",
    "#     input_dict = {'question': textbox}\n",
    "#     response = llm_chain.run(input_dict)\n",
    "\n",
    "#     print(\"user:\", textbox)\n",
    "#     print(\"bot:\", response)\n",
    "#     return response\n",
    "\n",
    "def chat_interface(textbox, chat):\n",
    "    input_dict = {'question': textbox}\n",
    "    response_dict = llm_chain.invoke(input_dict)\n",
    "    text = response_dict['text']  # Extract the text from the dictionary\n",
    "    # Split the text based on \"Response:\" and extract the part after it\n",
    "    # if \"Response:\" in text:\n",
    "    response_text = text.split(\"Response:\")[1].strip()\n",
    "    print(text)\n",
    "    return response_text\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=chat_interface,\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me a question\", container=False, scale=7),\n",
    "    title=\"Chatbot\",\n",
    "    description=\"Ask Chatbot any question\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\"What does AI stand for?\", \"What is Software Engineering?\", \"What is Cybersecurity?\"],\n",
    "    cache_examples=False,\n",
    "    retry_btn=None,\n",
    "    undo_btn=\"Delete Previous\",\n",
    "    clear_btn=\"Clear\",\n",
    ").launch()\n",
    "\n",
    "#I am a final year Computer Science student seeking to find a graduate role in __. What are practical skills required for a career in __?\n",
    "#I am a beginner that wants to get into __, where should I start?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load standardized test set\n",
    "    # IT Consultant, Cloud Engineer...\n",
    "\n",
    "# ROGUE? BLUE?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAYBE DO THIS FIRST? AND SEE THE DOWNSIDE, AND LEARN THAT IT IS NOT REQUIRED (doesn't solve hallucinations and timely context!)\n",
    "# Fine-tune with input and output example data sets\n",
    "\n",
    "# Compare with different models (one fine-tuned one just pre-trained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
